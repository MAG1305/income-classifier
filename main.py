"""
Main Script - Income Classifier with Spark ML
==============================================

This is the main entry point to run the complete income
classification analysis.

Usage:
    python main.py
"""

import sys
import os

# Add directories to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'config'))

from spark_config import create_spark_session, stop_spark_session
from income_classifier import IncomeClassifier
from utils import analyze_data_distribution, evaluate_model_performance, save_results_to_file, create_prediction_summary

def main():
    """
    Main function that runs the complete analysis
    """
    print("=" * 80)
    print("ğŸ¦ INCOME CLASSIFIER WITH SPARK ML - COMPLETE ANALYSIS")
    print("=" * 80)
    
    data_path = "data/adult_income_sample.csv"
    spark = None
    
    try:
        print("\nğŸš€ Initializing Spark...")
        spark = create_spark_session("IncomeClassifierComplete")
        print("âœ… Spark initialized successfully")

        if not os.path.exists(data_path):
            print(f"âŒ Error: Data file not found at {data_path}")
            return 1

        classifier = IncomeClassifier(data_path)
        classifier.spark = spark 
        
        print("\nğŸ“Š Loading and analyzing data...")
        classifier.load_data()
        
        print("\nğŸ” Performing exploratory analysis...")
        pandas_df = analyze_data_distribution(classifier.df, spark)
        
        print("\nğŸ”§ Preprocessing data...")
        preprocessing_stages = classifier.preprocess_data()
        
        print("\nğŸ¤– Creating and training model...")
        classifier.create_model(preprocessing_stages)
        predictions = classifier.train_model()
        
        print("\nğŸ“ˆ Evaluating model performance...")
        results = evaluate_model_performance(predictions)
        
        if results:
            print("\nğŸ’¾ Saving results...")
            save_results_to_file(results)
        
        create_prediction_summary(predictions, spark)
        
        print("\nğŸ†• Creating new data for prediction...")
        new_df = classifier.create_new_data()
        new_predictions = classifier.predict_new_data(new_df)
        
        print("\nğŸ‰ COMPLETE ANALYSIS FINISHED SUCCESSFULLY!")
        print("\nğŸ“Š Analysis summary:")
        print(f"   â€¢ Dataset: {classifier.df.count()} registros")
        if results:
            print(f"   â€¢ Model precision: {results['precision']:.3f}")
            print(f"   â€¢ Recall: {results['recall']:.3f}")
            print(f"   â€¢ F1-Score: {results['f1_score']:.3f}")
            print(f"   â€¢ Accuracy: {results['accuracy']:.3f}")
        
    except Exception as e:
        print(f"\nâŒ Error during analysis: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
        
    finally:
        if spark:
            stop_spark_session(spark)
            print("\nğŸ›‘ Spark session stopped")
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
