"""
Script Principal - Clasificador de Ingresos con Spark ML
=======================================================

Este es el punto de entrada principal para ejecutar todo el anÃ¡lisis
de clasificaciÃ³n de ingresos.

Uso:
    python main.py
"""

import sys
import os
from pyspark.sql import SparkSession

# Agregar directorios al path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'config'))

from income_classifier import IncomeClassifier
from utils import analyze_data_distribution, create_visualizations, evaluate_model_performance, save_results_to_file, create_prediction_summary
from spark_config import create_spark_session, stop_spark_session

def main():
    """
    FunciÃ³n principal que ejecuta todo el anÃ¡lisis
    """
    print("=" * 80)
    print("ğŸ¦ CLASIFICADOR DE INGRESOS CON SPARK ML - ANÃLISIS COMPLETO")
    print("=" * 80)
    
    data_path = "data/adult_income_sample.csv"
    spark = None
    
    try:
        print("\nğŸš€ Inicializando Spark...")
        spark = create_spark_session("IncomeClassifierComplete")
        print("âœ… Spark inicializado correctamente")
        
        classifier = IncomeClassifier(data_path)
        classifier.spark = spark 
        
        print("\nğŸ“Š Cargando y analizando datos...")
        classifier.load_data()
        
        print("\nğŸ” Realizando anÃ¡lisis exploratorio...")
        pandas_df = analyze_data_distribution(classifier.df, spark)
        
        print("\nğŸ“Š Creando visualizaciones...")
        try:
            create_visualizations(pandas_df)
        except Exception as e:
            print(f"âš ï¸  No se pudieron crear las visualizaciones: {e}")
            print("   (Esto es normal si no tienes matplotlib instalado)")
        
        print("\nğŸ”§ Preprocesando datos...")
        preprocessing_stages = classifier.preprocess_data()
        
        print("\nğŸ¤– Creando y entrenando modelo...")
        classifier.create_model(preprocessing_stages)
        predictions = classifier.train_model()
        
        print("\nğŸ“ˆ Evaluando rendimiento del modelo...")
        results = evaluate_model_performance(predictions)
        
        print("\nğŸ’¾ Guardando resultados...")
        save_results_to_file(results)
        
        create_prediction_summary(predictions, spark)
        
        print("\nğŸ†• Creando datos nuevos para predicciÃ³n...")
        new_df = classifier.create_new_data()
        new_predictions = classifier.predict_new_data(new_df)
        
        print("\nğŸ‰ ANÃLISIS COMPLETO FINALIZADO EXITOSAMENTE!")
        print("\nğŸ“ Archivos generados:")
        print("   â€¢ results/model_results.txt - MÃ©tricas del modelo")
        print("   â€¢ results/data_analysis.png - GrÃ¡ficos de anÃ¡lisis (si estÃ¡ disponible)")
        
        print("\nğŸ“Š Resumen del anÃ¡lisis:")
        print(f"   â€¢ Dataset: {classifier.df.count()} registros")
        print(f"   â€¢ PrecisiÃ³n del modelo: {results['precision']:.3f}")
        print(f"   â€¢ Sensibilidad: {results['recall']:.3f}")
        print(f"   â€¢ F1-Score: {results['f1_score']:.3f}")
        print(f"   â€¢ Exactitud: {results['accuracy']:.3f}")
        
    except Exception as e:
        print(f"\nâŒ Error durante el anÃ¡lisis: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
        
    finally:
        # Detener Spark
        if spark:
            stop_spark_session(spark)
            print("\nğŸ›‘ SesiÃ³n de Spark detenida")
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
