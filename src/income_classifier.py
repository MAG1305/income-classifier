"""
Clasificador de Ingresos con Spark ML
=====================================

Este script implementa un clasificador binario para predecir si una persona
gana mÃ¡s de 50K al aÃ±o usando regresiÃ³n logÃ­stica con Spark ML.

Autor: DataPros
Fecha: 2024
"""

import sys
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, isnan, count
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'config'))
from spark_config import create_spark_session, stop_spark_session

class IncomeClassifier:
    """
    Clasificador de ingresos usando Spark ML
    """
    
    def __init__(self, data_path):
        """
        Inicializa el clasificador
        
        Args:
            data_path (str): Ruta al archivo CSV con los datos
        """
        self.data_path = data_path
        self.spark = None
        self.df = None
        self.pipeline = None
        self.model = None
        
    def initialize_spark(self):
        """Inicializa la sesiÃ³n de Spark"""
        print("ðŸš€ Inicializando Spark...")
        self.spark = create_spark_session("IncomeClassifier")
        print("âœ… Spark inicializado correctamente")
        
    def load_data(self):
        """
        Carga los datos desde el archivo CSV
        
        Tarea 1: Carga de datos
        """
        print("\nðŸ“Š Cargando datos...")
        
        schema = StructType([
            StructField("age", IntegerType(), True),
            StructField("sex", StringType(), True),
            StructField("workclass", StringType(), True),
            StructField("fnlwgt", IntegerType(), True),
            StructField("education", StringType(), True),
            StructField("hours_per_week", IntegerType(), True),
            StructField("label", StringType(), True)
        ])
        
        self.df = self.spark.read \
            .option("header", "true") \
            .option("inferSchema", "false") \
            .schema(schema) \
            .csv(self.data_path)
        
        print(f"âœ… Datos cargados: {self.df.count()} registros")
        
        print("\nðŸ“‹ Esquema de los datos:")
        self.df.printSchema()
        
        print("\nðŸ“‹ Primeros 10 registros:")
        self.df.show(10, truncate=False)
        
        print("\nðŸ“ˆ EstadÃ­sticas descriptivas:")
        self.df.describe().show()
        
        print("\nðŸ” VerificaciÃ³n de valores nulos:")
        # Verificar valores nulos (solo para columnas numÃ©ricas)
        numeric_columns = ["age", "fnlwgt", "hours_per_week"]
        string_columns = ["sex", "workclass", "education", "label"]
        
        # Para columnas numÃ©ricas: verificar nulos y NaN
        numeric_nulls = self.df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in numeric_columns])
        print("Columnas numÃ©ricas (nulos y NaN):")
        numeric_nulls.show()
        
        # Para columnas de string: solo verificar nulos
        string_nulls = self.df.select([count(when(col(c).isNull(), c)).alias(c) for c in string_columns])
        print("Columnas de texto (nulos):")
        string_nulls.show()
        
    def preprocess_data(self):
        """
        Preprocesa las variables categÃ³ricas
        
        Tarea 2: Preprocesamiento de variables categÃ³ricas
        """
        print("\nðŸ”§ Preprocesando variables categÃ³ricas...")
        
        # Variables categÃ³ricas a procesar
        categorical_columns = ["sex", "workclass", "education", "label"]
        
        # Crear StringIndexers para cada variable categÃ³rica
        string_indexers = []
        for col_name in categorical_columns:
            indexer = StringIndexer(
                inputCol=col_name,
                outputCol=f"{col_name}_indexed",
                handleInvalid="keep"
            )
            string_indexers.append(indexer)
        
        # Crear OneHotEncoders para las variables categÃ³ricas (excepto label)
        one_hot_encoders = []
        for col_name in categorical_columns[:-1]:  # Excluir 'label'
            encoder = OneHotEncoder(
                inputCol=f"{col_name}_indexed",
                outputCol=f"{col_name}_encoded"
            )
            one_hot_encoders.append(encoder)
        
        # Crear VectorAssembler para combinar todas las caracterÃ­sticas
        # Tarea 3: Ensamblaje de caracterÃ­sticas
        feature_columns = ["age", "fnlwgt", "hours_per_week"]
        encoded_columns = [f"{col}_encoded" for col in categorical_columns[:-1]]
        all_features = feature_columns + encoded_columns
        
        vector_assembler = VectorAssembler(
            inputCols=all_features,
            outputCol="features"
        )
        
        # Crear el pipeline de preprocesamiento
        preprocessing_stages = string_indexers + one_hot_encoders + [vector_assembler]
        
        print("âœ… Preprocesamiento configurado")
        print(f"   - Variables categÃ³ricas indexadas: {categorical_columns}")
        print(f"   - Variables numÃ©ricas: {feature_columns}")
        print(f"   - CaracterÃ­sticas finales: {all_features}")
        
        return preprocessing_stages
    
    def create_model(self, preprocessing_stages):
        """
        Crea y configura el modelo de regresiÃ³n logÃ­stica
        
        Tarea 4: DefiniciÃ³n y entrenamiento del modelo
        """
        print("\nðŸ¤– Configurando modelo de RegresiÃ³n LogÃ­stica...")
        
        # Configurar regresiÃ³n logÃ­stica
        lr = LogisticRegression(
            featuresCol="features",
            labelCol="label_indexed",
            maxIter=100,
            regParam=0.01,
            elasticNetParam=0.8
        )
        
        # Crear pipeline completo
        self.pipeline = Pipeline(stages=preprocessing_stages + [lr])
        
        print("âœ… Modelo configurado")
        print("   - Algoritmo: RegresiÃ³n LogÃ­stica")
        print("   - MÃ¡ximo de iteraciones: 100")
        print("   - ParÃ¡metro de regularizaciÃ³n: 0.01")
        print("   - Elastic Net: 0.8")
        
    def train_model(self):
        """
        Entrena el modelo con todos los datos
        
        Tarea 5: EvaluaciÃ³n del modelo
        """
        print("\nðŸŽ¯ Entrenando modelo...")
        
        # Entrenar el modelo
        self.model = self.pipeline.fit(self.df)
        
        print("âœ… Modelo entrenado exitosamente")
        
        # Hacer predicciones
        predictions = self.model.transform(self.df)
        
        # Mostrar predicciones con probabilidades
        print("\nðŸ“Š Predicciones del modelo:")
        predictions.select(
            "age", "sex", "workclass", "education", "hours_per_week",
            "label", "label_indexed", "prediction", "probability"
        ).show(20, truncate=False)
        
        # Calcular mÃ©tricas de evaluaciÃ³n
        evaluator = BinaryClassificationEvaluator(
            labelCol="label_indexed",
            rawPredictionCol="rawPrediction"
        )
        
        auc = evaluator.evaluate(predictions)
        print(f"\nðŸ“ˆ MÃ©tricas de evaluaciÃ³n:")
        print(f"   - AUC: {auc:.4f}")
        
        # AnÃ¡lisis de resultados
        print("\nðŸ” AnÃ¡lisis de resultados:")
        print("   - El modelo ha sido entrenado con 2000 registros")
        print("   - Se utilizaron caracterÃ­sticas demogrÃ¡ficas y laborales")
        print("   - Las predicciones muestran la probabilidad de ganar >50K")
        
        return predictions
    
    def create_new_data(self):
        """
        Crea datos nuevos para predicciÃ³n
        
        Tarea 6: PredicciÃ³n con nuevos datos
        """
        print("\nðŸ†• Creando datos nuevos para predicciÃ³n...")
        
        # Crear DataFrame con datos nuevos
        new_data = [
            (25, "Male", "Private", 150000, "Bachelors", 40),  # Joven con educaciÃ³n universitaria
            (45, "Female", "Gov", 200000, "Masters", 35),      # Mujer con maestrÃ­a en gobierno
            (30, "Male", "Self-emp", 180000, "HS-grad", 50),   # Hombre autoempleado
            (55, "Female", "Private", 250000, "Bachelors", 30), # Mujer mayor con experiencia
            (22, "Male", "Private", 120000, "Some-college", 45), # Joven con educaciÃ³n parcial
            (40, "Female", "Gov", 220000, "Masters", 25),      # Mujer con maestrÃ­a, pocas horas
            (35, "Male", "Self-emp", 300000, "Bachelors", 60), # Hombre autoempleado, muchas horas
            (50, "Female", "Private", 280000, "HS-grad", 40),  # Mujer con experiencia
            (28, "Male", "Gov", 160000, "Bachelors", 35)       # Hombre joven en gobierno
        ]
        
        schema = StructType([
            StructField("age", IntegerType(), True),
            StructField("sex", StringType(), True),
            StructField("workclass", StringType(), True),
            StructField("fnlwgt", IntegerType(), True),
            StructField("education", StringType(), True),
            StructField("hours_per_week", IntegerType(), True)
        ])
        
        new_df = self.spark.createDataFrame(new_data, schema)
        
        print("âœ… Datos nuevos creados (9 registros)")
        print("\nðŸ‘€ Datos nuevos:")
        new_df.show(truncate=False)
        
        return new_df
    
    def predict_new_data(self, new_df):
        """
        Realiza predicciones con los datos nuevos
        """
        print("\nðŸ”® Realizando predicciones con datos nuevos...")
        
        # Hacer predicciones
        predictions = self.model.transform(new_df)
        
        # Mostrar resultados
        print("\nðŸ“Š Predicciones para datos nuevos:")
        results = predictions.select(
            "age", "sex", "workclass", "education", "hours_per_week",
            "prediction", "probability"
        ).collect()
        
        for i, row in enumerate(results, 1):
            prob = row["probability"][1]  # Probabilidad de clase >50K
            pred = ">50K" if row["prediction"] == 1.0 else "<=50K"
            print(f"\nPersona {i}:")
            print(f"  Edad: {row['age']}, Sexo: {row['sex']}, Trabajo: {row['workclass']}")
            print(f"  EducaciÃ³n: {row['education']}, Horas/semana: {row['hours_per_week']}")
            print(f"  PredicciÃ³n: {pred} (Probabilidad: {prob:.3f})")
        
        return predictions
    
    def run_complete_analysis(self):
        """
        Ejecuta el anÃ¡lisis completo
        """
        try:
            # Inicializar Spark
            self.initialize_spark()
            
            # Cargar datos
            self.load_data()
            
            # Preprocesar datos
            preprocessing_stages = self.preprocess_data()
            
            # Crear modelo
            self.create_model(preprocessing_stages)
            
            # Entrenar modelo
            predictions = self.train_model()
            
            # Crear datos nuevos
            new_df = self.create_new_data()
            
            # Predecir datos nuevos
            new_predictions = self.predict_new_data(new_df)
            
            print("\nðŸŽ‰ AnÃ¡lisis completado exitosamente!")
            
        except Exception as e:
            print(f"\nâŒ Error durante el anÃ¡lisis: {str(e)}")
            raise
            
        finally:
            # Detener Spark
            if self.spark:
                stop_spark_session(self.spark)
                print("\nðŸ›‘ SesiÃ³n de Spark detenida")

def main():
    """
    FunciÃ³n principal
    """
    print("=" * 60)
    print("ðŸ¦ CLASIFICADOR DE INGRESOS CON SPARK ML")
    print("=" * 60)
    
    # Ruta al archivo de datos
    data_path = "data/adult_income_sample.csv"
    
    # Crear y ejecutar clasificador
    classifier = IncomeClassifier(data_path)
    classifier.run_complete_analysis()

if __name__ == "__main__":
    main()
